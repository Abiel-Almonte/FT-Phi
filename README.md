# Finetuning Phi-2

Phi-2 is a Small Language Model (SLM) with 2.7 billion parameters pretrained on textbook quality data, synthetic data, and carefully selected web data. Ideally resulting in a model cheaper to run in comparison to models of similar performance. However Phi-2 was not instruction finetuned, leading to worse performance responding to prompts with high specificality. Whereas, Mistral Instruct v0.2 is a Large Language Model (LLM) that overcomes such shortcomings yet it is expensive to run. 

Thus, Instruction finetuning Phi-2 on a synthetic dataset generated by Mistral-Instruct-v0.2 may yield a model that is cost efficient and high performing on instruction oriented prompts.

## Frameworks/ libraries

- Pytorch Lightning
    - High flexibility 
- Ray Train
    - Distributed training on a cluster of machines (local or cloud)
    - Configurable Fully Shared Data Parallel strategy (FSDP)
        - Partitions model parameters and gradients to all GPUs. [More Info.](https://arxiv.org/pdf/2304.11277.pdf)
- Peft
    - Parameter Efficient Fine-Tuning, in specific Low Rank Adaptation (LoRA).
        - Training less parameters quicker with less VRAM while maintaining similar performance to Full Fine-Tuning. [More Info.](https://arxiv.org/pdf/2106.09685.pdf)

