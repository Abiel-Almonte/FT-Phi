# Finetuning Phi-2

Phi-2 is a Small Language Model (SLM) with 2.7 billion paramters pretrained on textbook quality data, synthetic data, and carefully selected web data. Ideally resulting in a model cheaper to run in comparision to models of similar preformance. However Phi-2 was not instruction finetuned, leading to worse preformance responding to prompts with high specificality. Wherease, Mistral Instruct v0.2 is a Large Language Model (LLM) overcomes such shortcomings yet it is expensive to run. 

Thus, Instruction finetuning Phi-2 on a synthetic dataset generated by Mistral-Instruct-v0.2 will yield a model that is cost efficent and high preforming on instruction oriented prompts.

## Frameworks/ libraries

- Pytorch Lightning
    - High flexiblity 
- Ray Train
    - Distributed training on a cluster of machines (local or cloud)
    - Configurable Fully Shared Data Parallel strategy (FSDP)
        - Partitions model parameters and gradients to all GPUs. [More Info.](https://arxiv.org/pdf/2304.11277.pdf)
- Peft
    - Parameter Efficient Fine-Tuning, in specific Low Rank Adaptation (LoRA).
        - Training less paramters quicker with less VRAM while maintaing similar preformance to Full Fine-Tuning. [More Info.](https://arxiv.org/pdf/2106.09685.pdf)

